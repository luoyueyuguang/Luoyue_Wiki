1. 线性回归与逻辑回归区别
 - 线性回归和逻辑回归都是**广义线性回归模型的特例**
 - 线性回归只能用于**回归问题**，逻辑回归用于**分类问题**（可由二分类推广至多分类）
 - 线性回归无联系函数或不起作用，逻辑回归的联系函数是**对数几率函数**，属于Sigmoid函数
 - 线性回归使用**最小二乘法**作为参数估计方法，逻辑回归使用**极大似然法**作为参数估计方法

| 特征        | 线性回归                                  | 逻辑回归                                               |
| :-------- | :------------------------------------ | :------------------------------------------------- |
| **任务类型**  | 用于回归问题，预测连续值输出，如房价、温度等。               | 用于分类问题，预测离散类别标签，如垃圾邮件识别、疾病诊断等。                     |
| **输出结果**  | 输出是连续的数值。                             | 输出是概率值，介于0和1之间，通常根据阈值（如0.5）来确定类别。                  |
| **数学原理**  | 假设因变量与自变量之间存在线性关系，通过最小二乘法等方法找到最佳拟合直线。 | 基于逻辑函数（Sigmoid函数）将线性组合的输出映射到0和1之间，通过最大似然估计等方法训练模型。 |
| **损失函数**  | 通常使用均方误差（MSE）作为损失函数。                  | 使用交叉熵损失函数（也称为对数似然损失）。                              |
| **模型解释性** | 模型具有较强的可解释性，系数表示自变量对因变量的影响程度。         | 系数表示自变量对因变量属于某一类别的相对影响，可通过概率解释。                    |
| **应用场景**  | 适用于因变量与自变量呈线性关系的连续数据预测。               | 适用于需要对数据进行分类的场景，尤其是二分类问题。                          |

---

2. 在数学上还有哪些回归，这些回归与线性和逻辑的区别
- **多项式回归**:用于捕捉数据中的非线性关系，通过在回归方程中添加自变量的高次幂来实现
- **支持向量回归（SVR）**：基于支持向量机的原理，通过寻找一个能够以最小误差拟合数据的超平面。
- **随机森林回归**：通过构建多个决策树并将其预测结果进行平均来提高预测准确性